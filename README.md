# Project Report: Real Time (kind of) Audio Generation via Conditioning on Freesound Data

This project implements a pipeline that collects audio data from Freesound, preprocesses it, trains a generative transformer model, and finally generates audio in real time controlled by MIDI input.

The process begins with data preprocessing (`preprocessing.ipynb`). The preprocessing component queries Freesound using a set of predefined search terms such as "helicopter," "radio," "rain," and "supersaw" (these can be changed ad libitum). Each query is paired with a duration filter (for example, sounds between 6 and 30 seconds long) and a specified number of results, such as 50 sounds per query. Once the queries are executed, the corresponding high-quality OGG preview files are downloaded and stored in a designated directory. The downloaded files are then processed using the Essentia library, where each file is converted to the WAV format. The audio data is segmented into chunks of five seconds; only those chunks that meet the full five-second length are retained. In the subsequent step, these WAV chunks are encoded into a compressed format with a ".dac" extension. Two directories are prepared to store the encoded files for training and validation purposes. Additionally, an Excel spreadsheet is generated, where each entry records the path of a .dac file along with its class label. The class label is defined by the original search query. For instance, if the query "helicopter" returns 50 sounds, all the sounds derived from that query are classified as "helicopter". This entire preprocessing stage ensures that the audio data is uniformly formatted and appropriately labeled for later stages.

Training the model is managed by `Train.ipynb` that builds on the preprocessed data. The training dataset is created using the metadata from the generated Excel files, where each data entry includes the file path to the corresponding .dac file and its associated class label. A custom PyTorch DataLoader is used to feed batches of data to a decoder-only transformer-based model. The model is designed so that each time step is represented by a stack of four tokens. During training, the sequence length (or context window) is eight times longer than what is used for inference. A learnable multi-embedding layer converts each of the four tokens into an m-dimensional vector, and these vectors are concatenated to form the input for the transformer. Rotary Position Embedding (RoPE) is applied to the query and key matrices within each transformer block to provide positional information. The transformer blocks themselves incorporate causal attention, meaning that each token only attends to previous tokens. After processing through a series of transformer blocks, a final linear layer transforms the output into logits corresponding to a DAC encoder vocabulary with 1024 discrete values per token. The training process utilizes cross-entropy loss computed over all tokens and employs the Adam optimizer for updating model weights. Periodic checkpoints are saved. Hyperparameters such as learning rate, batch size, number of layers, and model size are loaded from the `params_mini.yaml` configuration file. We have chosen a model size of 512, with 6 layers and 8 heads. Training for 500 epochs took approximately an hour on a laptop equipped with an RTX-3060.

Real-time audio inference is handled by `inference_modified` that loads the trained model and enables interactive audio generation using MIDI input. Upon loading the model checkpoint, the script initializes a MIDI listener thread that continuously monitors incoming MIDI messages using the mido library. When a MIDI note or control change is detected, the corresponding class index is updated in a global variable. This index determines which one-hot conditioning vector is generated. The conditioning vector is crucial because it directs the model to generate audio that matches the specified class, such as "helicopter". During inference, the model predicts the next set of tokens, which represent the DAC-encoded audio. The generated DAC codes are then fed into a DAC decoder to reconstruct the audio waveform. Real-time audio playback is managed by the sounddevice library, which streams the audio continuously as chunks are generated.