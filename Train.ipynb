{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9bb5a36d-0fea-4c17-b799-f7bae8b0db61",
   "metadata": {},
   "source": [
    "### chaptGPT specs   \n",
    "\n",
    "A decoder-only transformer in pytorch to predict 'next output' at each time step. \n",
    "\n",
    "Each time step t is represented by a vector of n=4 tokens from the Descript DAC encoder. \n",
    "The length of the sequence (context window) is Ti=86 for inference, and Tt=8*Ti for training. That is, the context window for training is 8 times the length of the context window for inference. \n",
    "The attention is \"causal\", looking only back in time, and the maximum look-back time for the attention blocks is Ti (even when the sequence is longer during training). That is, the masking matrix is *banded* - triangular to be causal, and limited in lookback which results in a diagonal band). This prevents much of the training on shortened context that happens when tokens are near the beginning of traning examples. \n",
    "\n",
    "The size of the vocabulary (the number of descrete values in each codebook) for each of the n tokens is V=1024. \n",
    "\n",
    "The dataloader will as is usual, supply batches in triplets  (input, target, conditioning info) where the size of each input and output is Tt*n (the sequence length times the number of tokens at each time step). The tokens are indexes for the vocabulary in the range of (0, V-1). The targets are shifted relative to the input sequence by 1 as is typical for networks the learn to predict the output for the next time step. \n",
    "\n",
    "The first layer in the architecture will be a learnable \"multiembedding\" layer that embeds each of the 4 tokens at each time step as an m-dimensional vector. The n m-dimensional vectors are concatenated to provide the n*m dimensional input embeddings for the transformer blocks at each time step. \n",
    "\n",
    "A positional code is is added to the K and Q matricies in each Transformer block using Rotary Position Embedding (RoPE).\n",
    "\n",
    "We use a stack of b transformer blocks that are standard (using layer norms, a relu for activation, and a forward expansion factor of 4 form the linear layer). Each transformer block consumes and produces a context window length sequence of m*n dimensional vectors. \n",
    "\n",
    "After the last transformer block, there is a linear layer that maps the m*n dimensional vectors to the output size which is V*n (the vocabulary size time the number of tokens stacked at each time step). These are the logits that will be fed to the softmax functions (one for each of the n to kens) that provide the probability distribtion across the vocabulary set. We use the criterion nn.CrossEntropyLoss() for computing the loss using the targets provided by the dataloader, and Adam for the optimizer.\n",
    "\n",
    "Again, at inference time, the fixed-length context window is shorter than the training sequence window length, and equal to the maximum look-back time of the attention blocks. The inference process takes the output produced at each time step (a stack of n tokens), and shift them in to a sliding window that is used for input for the next time step. The length of the sequences generated during inference is arbitrary and should be settable with a parameter. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8db8df-da27-4eb1-91c5-1d4945ff4161",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce614c-47d9-4e16-bee8-3fbcc556b08a",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad58d43c-b453-496f-8a43-f0a1722b8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "paramfile = 'params_mini.yaml' # 'params.yaml' #\n",
    "DEVICE='cuda' ##''cuda'\n",
    "start_epoch=0 # to start from a previous training checkpoint, otherwise must be 0\n",
    "verboselevel=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9760e14-8c2f-4a14-be94-f1ef300296ce",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67b59e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# and for creating a custom dataset and loader:\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import yaml\n",
    "import shutil\n",
    "\n",
    "from utils.utils import generate_mask, save_model, load_model, writeDACFile, interpolate_vectors\n",
    "from DACTransformer.RopeCondDACTransformer import RopeCondDACTransformer\n",
    "\n",
    "from dataloader.dataset import CustomDACDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76dcc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b85ef",
   "metadata": {},
   "source": [
    "### <font color='blue'> Derived parameters </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638ee684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_size is 512\n",
      "using TransformerClass = RopeCondDACTransformer\n",
      "basefname = out.e512.l6.h8\n",
      "outdir = runs/mini_test_01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs/mini_test_01/params.yaml'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data dir\n",
    "\n",
    "# Load YAML file\n",
    "with open(paramfile, 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "\n",
    "data_dir = params['data_dir']\n",
    "data_frames =  params['data_frames']\n",
    "validator_data_dir = params['validator_data_dir']\n",
    "validator_data_frames = params['validator_data_frames']\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dataset = CustomDACDataset(data_dir=data_dir, metadata_excel=data_frames, transforms=None)\n",
    "\n",
    "# ---------     for the transformer  --------------#\n",
    "vocab_size = params['vocab_size']\n",
    "num_tokens = params['num_tokens']\n",
    "\n",
    "cond_classes = dataset.get_num_classes() # 0\n",
    "cond_params = params['cond_params']\n",
    "cond_size = cond_classes + cond_params # num_classes + num params - not a FREE parameter!\n",
    "\n",
    "#embed_size = params['tblock_input_size'] -cond_size # 240 #32  # embed_size +cond_size must be divisible by num_heads and by num tokens\n",
    "embed_size = params['model_size']  # embed_size  must be divisible by num_heads and by num tokens\n",
    "print(f'embed_size is {embed_size}')\n",
    "\n",
    "Ti = params['Ti']\n",
    "Tt = params['Tt']\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "sequence_length = Tt  # For training\n",
    "\n",
    "num_layers = params['num_layers']\n",
    "num_heads = params['num_heads']\n",
    "forward_expansion = params['forward_expansion']\n",
    "dropout_rate = params['dropout_rate']\n",
    "learning_rate = params['learning_rate']\n",
    "num_epochs=params['num_epochs']\n",
    "\n",
    "experiment_name=params['experiment'] \n",
    "outdir = 'runs' + '/' + experiment_name\n",
    "basefname= 'out' + '.e' + str(embed_size) + '.l' + str(num_layers) + '.h' + str(num_heads) \n",
    "\n",
    "ErrorLogRate = params['ErrorLogRate'] #10\n",
    "checkpoint_interval = params['checkpoint_interval']\n",
    "\n",
    "\n",
    "\n",
    "TransformerClass =  globals().get(params['TransformerClass'])  \n",
    "\n",
    "print(f\"using TransformerClass = {params['TransformerClass']}\") \n",
    "print(f'basefname = {basefname}')\n",
    "print(f'outdir = {outdir}')\n",
    "\n",
    "###########################################################################\n",
    "# Ensure the destination directory exists\n",
    "#destination_dir = os.path.dirname(outdir + '/' + paramfile)\n",
    "#if not os.path.exists(destination_dir):\n",
    "#    os.makedirs(destination_dir)\n",
    "    \n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "shutil.copy(paramfile, outdir + '/params.yaml')  # copy whatever paramfile was used to outdir and name it params.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf3928",
   "metadata": {},
   "source": [
    "### <font color='blue'> Set up cuda. \n",
    "Without it, training runs about 10 times slower  \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ff0adcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memeory on cuda 0 is  6.21903872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEVICE == 'cuda' :\n",
    "    torch.cuda.device_count()\n",
    "    torch.cuda.get_device_properties(0).total_memory/1e9\n",
    "\n",
    "    device = torch.device(DEVICE) # if the docker was started with --gpus all, then can choose here with cuda:0 (or cpu)\n",
    "    torch.cuda.device_count()\n",
    "    print(f'memeory on cuda 0 is  {torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
    "else :\n",
    "    device=DEVICE\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7020c",
   "metadata": {},
   "source": [
    "### <font color='blue'> Load data \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a120dac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[568, 804,  10, 674],\n",
       "         [568, 804,  10, 674],\n",
       "         [568, 804,  10, 674],\n",
       "         ...,\n",
       "         [568, 804,  10, 674],\n",
       "         [568, 804,  10, 674],\n",
       "         [568, 804,  10, 674]]),\n",
       " tensor([[568, 804,  10, 674],\n",
       "         [568, 804,  10, 674],\n",
       "         [568, 804,  10, 674],\n",
       "         ...,\n",
       "         [568, 804,  10, 674],\n",
       "         [568, 804,  10, 674],\n",
       "         [568, 804,  10, 674]]),\n",
       " tensor([0., 1., 0., 0., nan, nan, nan, nan, nan, nan]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c684557b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Inputs shape: torch.Size([16, 575, 4])\n",
      "Targets shape: torch.Size([16, 575, 4])\n",
      "cvect shape: torch.Size([16, 10])\n",
      "cevect is tensor([[0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,        nan,        nan,\n",
      "                nan,        nan,        nan,        nan],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,        nan,        nan,\n",
      "                nan,        nan,        nan,        nan],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,        nan,        nan,\n",
      "                nan,        nan,        nan,        nan],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,        nan,        nan,\n",
      "                nan,        nan,        nan,        nan],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,        nan,        nan,\n",
      "                nan,        nan,        nan,        nan],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1991e-01, 4.9123e-01,\n",
      "         4.7428e-02, 8.9340e-02, 1.2365e-02, 8.3863e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,        nan,        nan,\n",
      "                nan,        nan,        nan,        nan],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,        nan,        nan,\n",
      "                nan,        nan,        nan,        nan],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,        nan,        nan,\n",
      "                nan,        nan,        nan,        nan],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.5389e-02, 5.2632e-01,\n",
      "         1.4568e-01, 1.3857e-03, 3.9609e-05, 6.6263e-01],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,        nan,        nan,\n",
      "                nan,        nan,        nan,        nan],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8386e-02, 8.5965e-01,\n",
      "         8.4783e-02, 1.3273e-02, 5.8024e-04, 6.8064e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,        nan,        nan,\n",
      "                nan,        nan,        nan,        nan],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.8317e-02, 4.9123e-01,\n",
      "         3.5773e-01, 4.4179e-01, 4.4275e-01, 4.5114e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,        nan,        nan,\n",
      "                nan,        nan,        nan,        nan],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,        nan,        nan,\n",
      "                nan,        nan,        nan,        nan]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Validator data set\n",
    "if validator_data_dir != None :\n",
    "    validator_dataset=CustomDACDataset(data_dir=validator_data_dir, metadata_excel=validator_data_frames)\n",
    "    validator_dataloader= DataLoader(validator_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# Test data dir\n",
    "for batch_idx, (inputs, targets, cvect) in enumerate(dataloader):\n",
    "    #pass\n",
    "    # Your training code here\n",
    "    # inputs: batch of input data of shape [batch_size, N, T-1]\n",
    "    # targets: corresponding batch of target data of shape [batch_size, N, T-1]\n",
    "    \n",
    "    if (batch_idx == 0) : \n",
    "        print(f\"Batch {batch_idx + 1}\")\n",
    "        print(f\"Inputs shape: {inputs.shape}\")\n",
    "        print(f\"Targets shape: {targets.shape}\")\n",
    "        print(f\"cvect shape: {cvect.shape}\")\n",
    "        print(f'cevect is {cvect}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0eb6c",
   "metadata": {},
   "source": [
    "### <font color='blue'> Make the mask \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c9b110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask.shape is torch.Size([575, 575])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [-inf, -inf, -inf,  ..., 0., -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., 0., 0., -inf],\n",
       "        [-inf, -inf, -inf,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = generate_mask(Tt, Ti).to(device)\n",
    "print(f'Mask.shape is {mask.shape}')\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c4b392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model with embed_size=512, cond_size=10\n",
      " ------------- input_size is embed_size + cond_size = 522\n",
      " ------------- embed_dim (512) must be divisible by num_heads (8)\n",
      "Setting up MultiEmbedding with vocab_size= 1024, embed_size= 512, num_codebooks= 4\n",
      "Setting up RotaryPositionalEmbedding with embed_size= 512, max_len= 575\n",
      "Total number of parameters: 16848896\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model, put it on the device\n",
    "#model = TransformerDecoder(embed_size, num_layers, num_heads, forward_expansion, dropout_rate, Tt, num_tokens, vocab_size).to(device)\n",
    "print(f'Creating model with embed_size={embed_size}, cond_size={cond_size}')\n",
    "\n",
    "if start_epoch == 0 : \n",
    "    model = TransformerClass(embed_size, num_layers, num_heads, forward_expansion, dropout_rate, Tt, cond_classes, num_tokens, vocab_size, cond_size, verboselevel).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    checkpoint_path = outdir+\"/\"+basefname+\"_chkpt_\"+str(start_epoch).zfill(4) +\".pth\"\n",
    "    print(f'in train, start_epoch = {start_epoch} and checkpoint_path = {checkpoint_path}')\n",
    "    assert os.path.exists(checkpoint_path), f\"{checkpoint_path} does not exist.\"\n",
    "    if start_epoch != 0 and checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading and creating model from {checkpoint_path}\")       \n",
    "        # Restore model weights\n",
    "        model, optimizer, _, vocab_size, num_tokens, cond_size = load_model(checkpoint_path,  TransformerClass, device)\n",
    "        #best_metric = checkpoint['best_metric']  # If you're tracking performance      \n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "   \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Count the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {num_params}')\n",
    "\n",
    "# Initialize SummaryWriter for tensorboard \n",
    "writer = SummaryWriter(outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85526818-66e9-4bf0-9a94-0334ecd39d61",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2f038",
   "metadata": {},
   "source": [
    "# <font color='blue'> Train !! \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb439d-6d8f-4b1c-a4fd-6acac2a07db6",
   "metadata": {},
   "source": [
    "### loss is average CE across all output tokens\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{n=1}^{N} \\text{CE}(x_n, y_n)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e7dbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([575, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "372dc21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.0000, 0.0000, 0.0000, 0.9321, 0.8421, 0.0188, 0.0000, 0.0000,\n",
       "        0.7295])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "dataset.extract_conditioning_vector(\"543127_441000.dac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19920362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10  (with max 500), loss: nan\n",
      "Validation Loss: nan\n",
      "train time for 10 epochs, was 80.67760300636292\n",
      "\n",
      "EPOCH 20  (with max 500), loss: nan\n",
      "Validation Loss: nan\n",
      "train time for 20 epochs, was 161.63511562347412\n",
      "\n",
      "EPOCH 30  (with max 500), loss: nan\n",
      "Validation Loss: nan\n",
      "train time for 30 epochs, was 242.74615621566772\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m## -----------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m## OK, let's do it!\u001b[39;00m\n\u001b[1;32m     96\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m---> 97\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasefname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataloader, num_epochs, device, outdir, basefname, start_epoch, checkpoint_path)\u001b[0m\n\u001b[1;32m     44\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), target_data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m## <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m ErrorLogRate \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dacformer/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dacformer/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dacformer/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train(model, optimizer, dataloader, num_epochs, device, outdir, basefname, start_epoch=0, checkpoint_path=None):\n",
    "    t0 = time.time()\n",
    "    max_epoch = start_epoch + num_epochs\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        for batch_idx, (input_data, target_data, cond_data) in enumerate(dataloader):\n",
    "            if verboselevel > 5 :\n",
    "                print(f' ---- submitting batch with input_data={input_data.shape}, target_data={target_data.shape}, cond_data={cond_data.shape}')\n",
    "            #print(f\"b{batch_idx} \", end='')\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Move inputs and targets to the device\n",
    "            input_data, target_data, cond_data = input_data.to(device), target_data.to(device), cond_data.to(device)\n",
    "            \n",
    "            if cond_size==0 :  #Ignore conditioning data\n",
    "                cond_expanded=None\n",
    "            else : \n",
    "                # for dataset exammples, expand the conditioning info across all time steps before passing to models\n",
    "                cond_expanded = cond_data.unsqueeze(1).expand(-1, input_data.size(1), -1)\n",
    "            \n",
    "            #print(f'    after loading a batch,  input_data.shape is {input_data.shape}, and cond_data.shape is {cond_data.shape}')\n",
    "            #print(f'    after loading a batch,  cond_expanded.shape is {cond_expanded.shape}')\n",
    "            #print(f'    after loading a batch,  mask.shape is {mask.shape}')\n",
    "            #print(f' model={model}')\n",
    "            \n",
    "            # torch.Size([batch_size, seq_len, num_tokens, vocab_size])\n",
    "            output = model(input_data, cond_expanded, mask)\n",
    "        \n",
    "            if verboselevel > 5 :\n",
    "                print(f' TTTTTTTT after training, output shape ={output.shape}, torch.Size([batch_size, seq_len, num_tokens, vocab_size])')\n",
    "                print(f' TTTTTTTT Passing to CRITERION with , output.reshape(-1, vocab_size) = {output.reshape(-1, vocab_size).shape} and target_data.reshape(-1) = {target_data.reshape(-1).shape}' )\n",
    "    \n",
    "            ##  this works, but is too verbose >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            ##      # Original shape: (batch_size, seq_len, num_tokens, vocab_size)\n",
    "            ##      output = output.reshape(batch_size, sequence_length * num_tokens, vocab_size)\n",
    "            ##      # Original shape: (batch_size, seq_len, num_tokens)\n",
    "            ##      targets = targets.reshape(batch_size, sequence_length * num_tokens)\n",
    "            ##      loss = criterion(output.permute(0, 2, 1), targets) \n",
    "            \n",
    "            ##  more succinct <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "            #   Computes the CE for each token separately, and then averages them to get the loss.\n",
    "            #loss = criterion(output.reshape(-1, vocab_size), target_data.reshape(-1)) # collapses all target_data dimensions into a single dimension\n",
    "            loss = criterion(output.reshape(-1, vocab_size), target_data.reshape(-1).long())\n",
    "            ## <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch+1) % ErrorLogRate == 0:\n",
    "            print(f'EPOCH {epoch+1}  (with max {max_epoch}), ', end='')\n",
    "            print(f'loss: {loss}')\n",
    "            # Log the loss to TensorBoard\n",
    "            writer.add_scalar('Loss/train', loss, epoch)\n",
    "            \n",
    "            if validator_data_dir != None :\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0\n",
    "                    for val_inputs, val_targets, cond_data in validator_dataloader:\n",
    "                        val_inputs, val_targets, cond_data = val_inputs.to(device), val_targets.to(device), cond_data.to(device)\n",
    "                        \n",
    "                        if cond_size==0 :  #Ignore conditioning data\n",
    "                            cond_expanded=None\n",
    "                        else: \n",
    "                            # for dataset exammples, expand the conditioning info across all time steps before passing to models\n",
    "                            cond_expanded = cond_data.unsqueeze(1).expand(-1, input_data.size(1), -1)\n",
    "    \n",
    "                        \n",
    "                        val_outputs = model(val_inputs,cond_expanded, mask)\n",
    "                        \n",
    "                        val_loss += criterion(val_outputs.reshape(-1, vocab_size), val_targets.reshape(-1).long()) # collapses all target_data dimensions into a single dimension\n",
    "                        #val_loss += criterion(val_outputs, val_targets).item()\n",
    "    \n",
    "                print(f'Validation Loss: {val_loss / len(validator_dataloader)}')\n",
    "                writer.add_scalar('Loss/validation', val_loss / len(validator_dataloader), epoch)\n",
    "    \n",
    "                t1 = time.time()\n",
    "                train_time = t1-t0\n",
    "                print(f'train time for {epoch-start_epoch+1} epochs, was {train_time}' )\n",
    "                print(f'')\n",
    "                \n",
    "        if (epoch+1) % checkpoint_interval == 0:\n",
    "            lastbasename = outdir+\"/\"+basefname+\"_chkpt_\"+str(epoch+1).zfill(4)\n",
    "            print(f'EPOCH {epoch+1} save model to : {lastbasename}.pth')\n",
    "            print(f'')\n",
    "            save_model(model, optimizer, Ti,  lastbasename +\".pth\")\n",
    "        \n",
    "    \n",
    "    t1 = time.time()\n",
    "    train_time = t1-t0\n",
    "    print(f'train time for {num_epochs} epochs, was {train_time}' )\n",
    "    print(f'loss  =  {loss}' )\n",
    "    \n",
    "## -----------------------------------------------------------------------------------\n",
    "## OK, let's do it!\n",
    "num_epochs = 500\n",
    "train(model, optimizer, dataloader, num_epochs, device, outdir, basefname, start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16eb43b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just check that inference attention mask will look right\n",
    "#Actually, the inference mask can be None since we are using a context window only as long as the maximum look-back in the training mask\n",
    "# thats why taking the mask with :TI is upper-triangular. Longer dims would show a banded mask again.\n",
    "foo=mask[:Ti, :Ti]\n",
    "foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a1f25-78a6-479b-8771-2910cf639d67",
   "metadata": {},
   "source": [
    "### <font color='blue'> Use Inference.Decode.ipynb to see and hear your generated audio   \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68274809-4986-4d54-a442-ba7ac8b48ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
