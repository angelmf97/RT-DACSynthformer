{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45a40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from utils.utils import generate_mask, load_model, writeDACFile, sample_top_n\n",
    "from dataloader.dataset import CustomDACDataset\n",
    "from utils.utils import interpolate_vectors, breakpoints, breakpoints_classseq\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from DACTransformer.RopeCondDACTransformer import RopeCondDACTransformer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dac\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be7109-af19-4766-b0c8-4f800223ded0",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3286900-9141-49a6-bcb2-59b37836e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## params ##########################################################\n",
    "# set this to whatever you called the experiment in the top of your params.yaml file.\n",
    "experiment_name= \"mini_test_01\" #\"smalltest_dataset\" \n",
    "# probably don't change this is the default, set in the params.yaml file.\n",
    "checkpoint_dir = 'runs' + '/' + experiment_name  \n",
    "\n",
    "cptnum =  100 # (the checkpoint number must be in the checkpoint directory)\n",
    "SAVEWAV=False\n",
    "DEVICE='cuda' #######''cuda'\n",
    "gendur=20 #how many seconds you wnat your output sound to be\n",
    "topn=20 # sample from the top n logits\n",
    "device = DEVICE\n",
    "###########################################################################\n",
    "#  Choose a breakpoint sequence (and/or make one yourself) ...\n",
    "###########################################################################\n",
    "morphname='conditioning'  ###   (choose from breakpoint sets defined below)\n",
    "#morphname='sweep'  ###   (choose from breakpoint sets defined below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9808153c-d60e-4ce5-9146-f46e36e1b4c8",
   "metadata": {},
   "source": [
    "### Read Paramfile and get class list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c5cda0-aa32-47e5-a4cd-1f3f1bd3cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use paramfile= runs/mini_test_01/params.yaml\n",
      "classes=['helicopter', 'radio', 'rain', 'supersaw']\n",
      " ------- One hot vectors for classes ----------\n",
      " helicopter : \ttensor([1., 0., 0., 0.])\n",
      " radio : \ttensor([0., 1., 0., 0.])\n",
      " rain : \ttensor([0., 0., 1., 0.])\n",
      " supersaw : \ttensor([0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "#any config.yaml files used for training are copied to the checkpoint directory as \"params.yaml\"\n",
    "paramfile = checkpoint_dir + '/' +  'params.yaml' \n",
    "print(f\"will use paramfile= {paramfile}\") \n",
    "# Load YAML file\n",
    "with open(paramfile, 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "\n",
    "# Create an instance of the dataset\n",
    "data_dir = params['data_dir']\n",
    "data_frames =  params['data_frames']\n",
    "dataset = CustomDACDataset(data_dir=data_dir, metadata_excel=data_frames, transforms=None)\n",
    "\n",
    "#For your reference:\n",
    "#Print the list of all classes\n",
    "classes=dataset.get_class_list()\n",
    "print(f'classes={classes}')\n",
    "print(f' ------- One hot vectors for classes ----------')\n",
    "for i in range(len(classes)):\n",
    "    print(f' {classes[i]} : \\t{dataset.onehot(classes[i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05903b1",
   "metadata": {},
   "source": [
    "Morph over a vectors in vsequence lineary for (noramlized) time steps vtimes. Create your sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fa6fb-0f5f-4f02-82bf-46394645da80",
   "metadata": {},
   "source": [
    "### <font color='blue'> Derived parameters  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e30f7e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using TransformerClass = RopeCondDACTransformer\n",
      " and TransformerClass is class object <class 'DACTransformer.RopeCondDACTransformer.RopeCondDACTransformer'>\n",
      "embed_size is 512\n",
      "checkpoint_path = runs/mini_test_01/out.e512.l6.h8_chkpt_0100.pth, fnamebase = out.e512.l6.h8_chkpt_0100\n"
     ]
    }
   ],
   "source": [
    "# Get parameters from yaml file and derive any necessary\n",
    "######################################################\n",
    "\n",
    "inference_steps=86*gendur  #86 frames per second\n",
    "    \n",
    "TransformerClass =  globals().get(params['TransformerClass'])  \n",
    "print(f\"using TransformerClass = {params['TransformerClass']}\") \n",
    "print(f' and TransformerClass is class object {TransformerClass}')\n",
    "\n",
    "cond_size = 8 # num_classes + num params - not a FREE parameter!\n",
    "\n",
    "### embed_size = params['tblock_input_size'] -cond_size # 240 #32  # embed_size must be divisible by num_heads and by num tokens\n",
    "embed_size = params['model_size'] # 240 #32  # embed_size must be divisible by num_heads and by num tokens\n",
    "print(f'embed_size is {embed_size}')\n",
    "\n",
    "\n",
    "fnamebase='out' + '.e' + str(embed_size) + '.l' + str(params['num_layers']) + '.h' + str(params['num_heads']) + '_chkpt_' + str(cptnum).zfill(4) \n",
    "checkpoint_path = checkpoint_dir + '/' +  fnamebase  + '.pth' \n",
    "\n",
    "# for saving sound \n",
    "outdir=checkpoint_dir\n",
    "\n",
    "print(f'checkpoint_path = {checkpoint_path}, fnamebase = {fnamebase}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59747826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memeory on cuda 0 is  6.21903872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEVICE == 'cuda' :\n",
    "    torch.cuda.device_count()\n",
    "    torch.cuda.get_device_properties(0).total_memory/1e9\n",
    "\n",
    "    device = torch.device(DEVICE) # if the docker was started with --gpus all, then can choose here with cuda:0 (or cpu)\n",
    "    torch.cuda.device_count()\n",
    "    print(f'memeory on cuda 0 is  {torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
    "else :\n",
    "    device=DEVICE\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344f167",
   "metadata": {},
   "source": [
    "# The inference method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d6577fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import threading\n",
    "import mido\n",
    "\n",
    "# Shared class index variable, updated by MIDI thread\n",
    "current_class_idx = 0  # Default to the first class (update via MIDI)\n",
    "\n",
    "def midi_listener(port_name, num_classes):\n",
    "    \"\"\"\n",
    "    Listens for MIDI input and updates `current_class_idx` dynamically.\n",
    "    \n",
    "    Args:\n",
    "        port_name (str): The name of the MIDI port to listen on.\n",
    "        num_classes (int): Number of available classes.\n",
    "    \"\"\"\n",
    "    global current_class_idx\n",
    "\n",
    "    try:\n",
    "        with mido.open_input(port_name) as port:\n",
    "            print(f\"Listening for MIDI on {port_name}...\")\n",
    "            for msg in port:\n",
    "                if msg.type == 'note_on':  # Use note number to pick a class\n",
    "                    new_class = msg.note % num_classes  # Map MIDI notes to classes\n",
    "                    current_class_idx = new_class\n",
    "                    print(f\"Updated class index: {current_class_idx}\")\n",
    "                \n",
    "                elif msg.type == 'control_change':  # Use CC for a different mapping\n",
    "                    new_class = msg.value % num_classes\n",
    "                    current_class_idx = new_class\n",
    "                    print(f\"Updated class index via CC: {current_class_idx}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"MIDI Error: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f33f63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for MIDI on Launchkey Mini MK3:Launchkey Mini MK3 Launchkey Mi 28:0...\n"
     ]
    }
   ],
   "source": [
    "# Print available midi ports\n",
    "port = mido.get_input_names()[1]\n",
    "# Start MIDI listener thread\n",
    "midi_thread = threading.Thread(target=midi_listener, args=(port, len(classes)))  # Replace \"YourMIDIport\" with actual port\n",
    "midi_thread.daemon = True\n",
    "midi_thread.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efadad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_midi_controlled_cond(inference_steps, classes, param_count=1):\n",
    "    \"\"\"\n",
    "    Generates a conditioning sequence where the class index is controlled by live MIDI input.\n",
    "    \n",
    "    Args:\n",
    "        inference_steps (int): Number of time steps (frames) for inference.\n",
    "        classes (list): List of class names (one-hot encoded).\n",
    "        param_count (int): Number of continuous parameters (random walk).\n",
    "    \n",
    "    Returns:\n",
    "        cond: A Tensor of shape (1, inference_steps, cond_size).\n",
    "    \"\"\"\n",
    "    num_classes = len(classes)\n",
    "    cond_size = num_classes + param_count\n",
    "\n",
    "    # Prepare a buffer for (inference_steps, cond_size)\n",
    "    cond = torch.zeros(inference_steps, cond_size)\n",
    "\n",
    "    for p in range(param_count):\n",
    "        cond[0, num_classes + p] = .5\n",
    "\n",
    "    for t in range(1, inference_steps):\n",
    "        # Update class index from MIDI input (global variable)\n",
    "        global current_class_idx\n",
    "\n",
    "        # Copy previous step\n",
    "        cond[t] = cond[t-1]\n",
    "\n",
    "        # Reset the class portion to zero, then set the one-hot class\n",
    "        cond[t, :num_classes] = 0.0\n",
    "        cond[t, current_class_idx] = 1.0\n",
    "        \n",
    "        # Interpolate cond with previous cond using torch.lerp\n",
    "        alpha = 0.9\n",
    "        # prev_cond.to(device)\n",
    "        # cond.to(device)\n",
    "        # cond[t] = torch.lerp(prev_cond, cond[t], alpha)\n",
    "\n",
    "    # Add batch dimension => shape (1, T, cond_size)\n",
    "    return cond.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aedbff75",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def inference(model, inference_cond, Ti_context_length, vocab_size, num_tokens, inference_steps, topn, fname) :\n",
    "    model.eval()\n",
    "    mask = generate_mask(Ti_context_length, Ti_context_length).to(device)\n",
    "\n",
    "    input_data = torch.randint(0, vocab_size, (1, Ti_context_length, num_tokens)).to(device)  # Smaller context window for inference\n",
    "    \n",
    "    #Extend the first conditional vector to cover the \"input\" which is of length Ti_context_length\n",
    "    inference_cond = torch.cat([inference_cond[:, :1, :].repeat(1, Ti_context_length, 1), inference_cond], dim=1)\n",
    "    predictions = []\n",
    "\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for i in range(inference_steps):  # \n",
    "        if cond_size == 0:\n",
    "            output = model(input_data, None, mask) # step through \n",
    "        else : \n",
    "            output = model(input_data, inference_cond[:, i:Ti_context_length+i, :], mask) # step through\n",
    "\n",
    "        # This takes the last vector of the sequence (the new predicted token stack) so has size(b,steps,4,1024)\n",
    "        # This it takes the max across the last dimension (scores for each element of the vocabulary (for each of the 4 tokens))\n",
    "        # .max returns a duple of tensors, the first are the max vals (one for each token) and the second are the\n",
    "        #        indices in the range of the vocabulary size. \n",
    "        # THAT IS, the selected \"best\" tokens (one for each codebook) are taken independently\n",
    "        ########################### next_token = output[:, -1, :, :].max(-1)[1]  # Greedy decoding for simplicity\n",
    "        \n",
    "        next_token = sample_top_n(output[:, -1, :, :],topn) # topn=1 would be the same as max in the comment line above    \n",
    "        predictions.append(next_token)\n",
    "        input_data = torch.cat([input_data, next_token.unsqueeze(1)], dim=1)[:, 1:]  # Slide window\n",
    "\n",
    "    t1 = time.time()\n",
    "    inf_time = t1-t0\n",
    "\n",
    "    dacseq = torch.cat(predictions, dim=0).unsqueeze(0).transpose(1, 2)\n",
    "\n",
    "    return dacseq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6637fd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------------- embed_dim (512) must be divisible by num_heads (8)\n",
      "Setting up MultiEmbedding with vocab_size= 1024, embed_size= 512, num_codebooks= 4\n",
      "Setting up RotaryPositionalEmbedding with embed_size= 512, max_len= 430\n",
      "Mode loaded, context_length (Ti_context_length) = 86\n",
      "Total number of parameters: 16833536\n"
     ]
    }
   ],
   "source": [
    "#Load the stored model\n",
    "model, _, Ti_context_length, vocab_size, num_codebooks, cond_size = load_model(checkpoint_path,  TransformerClass, DEVICE)\n",
    "\n",
    "print(f'Mode loaded, context_length (Ti_context_length) = {Ti_context_length}')\n",
    "# Count the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {num_params}')\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2828aef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DAC decoder is in /home/angel/.cache/descript/dac/weights_44khz_8kbps_0.0.1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/anaconda3/envs/dacformer/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "dacmodel_path = dac.utils.download(model_type=\"44khz\") \n",
    "print(f'The DAC decoder is in {dacmodel_path}')\n",
    "with torch.no_grad():\n",
    "    dacmodel = dac.DAC.load(dacmodel_path)\n",
    "\n",
    "    dacmodel.to(device); #wanna see the model? remove the semicolon\n",
    "    dacmodel.eval();  # need to be \"in eval mode\" in order to set the number of quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sounddevice as sd\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Initialize audio queue\n",
    "try:\n",
    "    del audio_queue\n",
    "except:\n",
    "    pass\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Audio callback function\n",
    "def audio_callback(outdata, frames, time_info, status):\n",
    "    if status:\n",
    "        print(f\"Status: {status}\")\n",
    "    try:\n",
    "        chunk = audio_queue.get_nowait()\n",
    "        outdata[:len(chunk)] = chunk.reshape(-1, 1)\n",
    "        if len(chunk) < frames:\n",
    "            outdata[len(chunk):] = 0\n",
    "    except queue.Empty:\n",
    "        outdata.fill(0)\n",
    "\n",
    "dur = 1\n",
    "blocksize = int(44100 * dur)\n",
    "inference_steps = int(86 * dur)\n",
    "\n",
    "# Function to generate audio using the model\n",
    "def generate_audio():\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            print(f'current_class = {classes[current_class_idx]}')\n",
    "            cond = generate_midi_controlled_cond(\n",
    "                inference_steps, \n",
    "                classes, \n",
    "                param_count=1).to(device)   \n",
    "\n",
    "            # Model inference\n",
    "            codeseq = inference(model, cond, Ti_context_length, vocab_size, num_codebooks, inference_steps, topn, \"\")\n",
    "            dac_file = dac.DACFile(\n",
    "                codes=codeseq.cpu(),\n",
    "                chunk_length=codeseq.shape[2],\n",
    "                original_length=int(codeseq.shape[2] * 512),\n",
    "                input_db=torch.tensor(-20),\n",
    "                channels=1,\n",
    "                sample_rate=44100,\n",
    "                padding=True,\n",
    "                dac_version='1.0.0'\n",
    "            )\n",
    "            audio_signal = dacmodel.decompress(dac_file)\n",
    "            audio_data = audio_signal.samples.view(-1).numpy()\n",
    "            \n",
    "            # Enqueue audio data\n",
    "            # Slice the audio_data into 4096 frames blocks\n",
    "            audio_queue.put(audio_data[:blocksize])\n",
    "            # for i in range(0, len(audio_data), blocksize):\n",
    "            #     chunk = audio_data[i:i+blocksize]\n",
    "            #     audio_queue.put(chunk)\n",
    "\n",
    "# Start audio stream\n",
    "samplerate = 44100\n",
    "stream = sd.OutputStream(\n",
    "    samplerate=samplerate,\n",
    "    channels=1,\n",
    "    blocksize=blocksize,\n",
    "    callback=audio_callback\n",
    ")\n",
    "stream.start()\n",
    "\n",
    "# Start audio generation in a separate thread\n",
    "audio_thread = threading.Thread(target=generate_audio)\n",
    "# audio_thread.daemon = True\n",
    "audio_thread.start()\n",
    "\n",
    "# Keep the main thread alive while audio is playing\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping...\")\n",
    "finally:\n",
    "    stream.stop()\n",
    "    stream.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
